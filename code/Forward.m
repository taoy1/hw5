function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.

act_a = cell(length(W),1);
act_h = cell(length(W),1);

for i = 1:length(W) % numLayers-1
    n = length(b{i});
    act_a{i} = zeros(n,1);
    
    if i == 1
        last = X;
    else
        last = act_h{i-1};
    end
    
    % pre-activation
    act_a{i} = sum(W{i} .* repmat(last, [1, size(W{i},2)]))' + b{i};
    
    % post-activation
    if i ~= length(W)
        act_h{i} = ones(n,1)./(1+exp(-act_a{i}));
    else
        act_h{i} = exp(act_a{i}) / sum(exp(act_a{i}));
    end
end

output = act_h{length(W)};

