function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.

act_a = cell(length(W),1);
act_h = cell(length(W),1);

for i = 1:length(W) % numLayers-1
    n = length(b{i});
    act_a{i} = zeros(n,1);
    
    for j = 1:n % numNodes of output
        % pre-activation
        w = W{i}(:,j); % w at layer i -> i+1 and connecting to node j
        if i==1
            act_a{i}(j) = sum(w.*X) + b{i}(j);
        else
            act_a{i}(j) = sum(w.*act_h{i-1}) + b{i}(j);
        end
    end
    % post-activation
    if i ~= length(W)
        act_h{i} = ones(n,1)./(1+exp(-act_a{i}));
    else
        act_h{i} = exp(act_a{i}) / sum(exp(act_a{i}));
    end
end

output = act_h{length(W)};

